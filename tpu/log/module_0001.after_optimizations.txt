HloModule cluster_1__XlaCompiledKernel_true__XlaNumConstantArgs_5__XlaNumResourceArgs_0_.76

%owlnode52-reduction.49 (x.50: f32[], y.51: f32[]) -> f32[] {
  %x.50 = f32[] parameter(0)
  %y.51 = f32[] parameter(1)
  ROOT %maximum.52 = f32[] maximum(f32[] %x.50, f32[] %y.51)
}

%owlnode57-reduction.63 (x.64: f32[], y.65: f32[]) -> f32[] {
  %x.64 = f32[] parameter(0)
  %y.65 = f32[] parameter(1)
  ROOT %add.66 = f32[] add(f32[] %x.64, f32[] %y.65)
}

%fused_computation (param_0: f32[100,10], param_1.1: f32[100]) -> f32[100,10] {
  %param_0 = f32[100,10]{1,0} parameter(0)
  %param_1.1 = f32[100]{0} parameter(1)
  %broadcast.3 = f32[100,10]{1,0} broadcast(f32[100]{0} %param_1.1), dimensions={0}, metadata={op_type="Div" op_name="owlnode59"}
  ROOT %divide.0 = f32[100,10]{1,0} divide(f32[100,10]{1,0} %param_0, f32[100,10]{1,0} %broadcast.3), metadata={op_type="Div" op_name="owlnode59"}
}

%fused_computation.1 (param_0.2: f32[100,10], param_1.3: f32[100]) -> f32[100,10] {
  %param_0.2 = f32[100,10]{1,0} parameter(0)
  %param_1.3 = f32[100]{0} parameter(1)
  %broadcast.4 = f32[100,10]{1,0} broadcast(f32[100]{0} %param_1.3), dimensions={0}, metadata={op_type="Sub" op_name="owlnode53"}
  %subtract.0 = f32[100,10]{1,0} subtract(f32[100,10]{1,0} %param_0.2, f32[100,10]{1,0} %broadcast.4), metadata={op_type="Sub" op_name="owlnode53"}
  ROOT %exponential.0 = f32[100,10]{1,0} exponential(f32[100,10]{1,0} %subtract.0), metadata={op_type="Exp" op_name="owlnode55"}
}

%fused_computation.2 (param_0.3: f32[100,10], param_1.5: f32[100,1024], param_2: f32[1024,10]) -> f32[100,10] {
  %param_0.3 = f32[100,10]{1,0} parameter(0)
  %param_1.5 = f32[100,1024]{1,0} parameter(1)
  %param_2 = f32[1024,10]{1,0} parameter(2)
  %dot.0 = f32[100,10]{1,0} dot(f32[100,1024]{1,0} %param_1.5, f32[1024,10]{1,0} %param_2), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="MatMul" op_name="owlnode48"}
  ROOT %add.0 = f32[100,10]{1,0} add(f32[100,10]{1,0} %param_0.3, f32[100,10]{1,0} %dot.0), metadata={op_type="Add" op_name="owlnode50"}
}

%fused_computation.3 (param_0.5: f32[100,1024], param_1.9: f32[1,1024]) -> f32[100,1024] {
  %constant_1 = f32[] constant(0), metadata={op_type="Relu" op_name="owlnode32"}
  %broadcast.6 = f32[100,1024]{1,0} broadcast(f32[] %constant_1), dimensions={}, metadata={op_type="Relu" op_name="owlnode44"}
  %param_1.9 = f32[1,1024]{1,0} parameter(1)
  %bitcast.6 = f32[1024]{0} bitcast(f32[1,1024]{1,0} %param_1.9), metadata={op_type="Add" op_name="owlnode42"}
  %broadcast.5 = f32[100,1024]{1,0} broadcast(f32[1024]{0} %bitcast.6), dimensions={1}, metadata={op_type="Add" op_name="owlnode42"}
  %param_0.5 = f32[100,1024]{1,0} parameter(0)
  %add.1 = f32[100,1024]{1,0} add(f32[100,1024]{1,0} %broadcast.5, f32[100,1024]{1,0} %param_0.5), metadata={op_type="Add" op_name="owlnode42"}
  ROOT %maximum.0 = f32[100,1024]{1,0} maximum(f32[100,1024]{1,0} %broadcast.6, f32[100,1024]{1,0} %add.1), metadata={op_type="Relu" op_name="owlnode44"}
}

%max_F32.25 (lhs.26: f32[], rhs.27: f32[]) -> f32[] {
  %lhs.26 = f32[] parameter(0)
  %rhs.27 = f32[] parameter(1)
  ROOT %maximum.28 = f32[] maximum(f32[] %lhs.26, f32[] %rhs.27)
}

%fused_computation.4 (param_0.10: f32[100,32,28,28], param_1.13: f32[32]) -> f32[100,6272] {
  %constant_3 = f32[] constant(0), metadata={op_type="Relu" op_name="owlnode32"}
  %broadcast.8 = f32[100,32,14,14]{3,2,1,0} broadcast(f32[] %constant_3), dimensions={}, metadata={op_type="Relu" op_name="owlnode32"}
  %param_0.10 = f32[100,32,28,28]{3,2,1,0} parameter(0)
  %param_1.13 = f32[32]{0} parameter(1)
  %broadcast.7 = f32[100,32,28,28]{3,2,1,0} broadcast(f32[32]{0} %param_1.13), dimensions={1}, metadata={op_type="Add" op_name="owlnode30"}
  %add.2 = f32[100,32,28,28]{3,2,1,0} add(f32[100,32,28,28]{3,2,1,0} %param_0.10, f32[100,32,28,28]{3,2,1,0} %broadcast.7), metadata={op_type="Add" op_name="owlnode30"}
  %constant_2 = f32[] constant(-inf), metadata={op_type="MaxPool" op_name="owlnode34"}
  %reduce-window.0 = f32[100,32,14,14]{3,2,1,0} reduce-window(f32[100,32,28,28]{3,2,1,0} %add.2, f32[] %constant_2), window={size=1x1x2x2 stride=1x1x2x2}, to_apply=%max_F32.25, metadata={op_type="MaxPool" op_name="owlnode34"}
  %maximum.1 = f32[100,32,14,14]{3,2,1,0} maximum(f32[100,32,14,14]{3,2,1,0} %broadcast.8, f32[100,32,14,14]{3,2,1,0} %reduce-window.0), metadata={op_type="Relu" op_name="owlnode32"}
  %bitcast.8 = f32[100,14,14,32]{2,1,3,0} bitcast(f32[100,32,14,14]{3,2,1,0} %maximum.1), metadata={op_type="Transpose" op_name="owlnode32-0-0-TransposeNCHWToNHWC-LayoutOptimizer"}
  %copy.6 = f32[100,14,14,32]{3,2,1,0} copy(f32[100,14,14,32]{2,1,3,0} %bitcast.8), metadata={op_type="Transpose" op_name="owlnode32-0-0-TransposeNCHWToNHWC-LayoutOptimizer"}
  ROOT %bitcast.7 = f32[100,6272]{1,0} bitcast(f32[100,14,14,32]{3,2,1,0} %copy.6), metadata={op_type="Reshape" op_name="owlnode36"}
}

%fused_computation.5 (param_0.13: f32[100,28,28,1]) -> f32[100,1,28,28] {
  %constant_4 = f32[] constant(0.00390625), metadata={op_type="Mul" op_name="owlnode27"}
  %broadcast.9 = f32[100,28,28,1]{2,1,0,3} broadcast(f32[] %constant_4), dimensions={}, metadata={op_type="Mul" op_name="owlnode27"}
  %param_0.13 = f32[100,28,28,1]{3,2,1,0} parameter(0)
  %bitcast.10 = f32[100,28,28,1]{2,1,0,3} bitcast(f32[100,28,28,1]{3,2,1,0} %param_0.13), metadata={op_name="XLA_Args"}
  %multiply.0 = f32[100,28,28,1]{2,1,0,3} multiply(f32[100,28,28,1]{2,1,0,3} %broadcast.9, f32[100,28,28,1]{2,1,0,3} %bitcast.10), metadata={op_type="Mul" op_name="owlnode27"}
  ROOT %bitcast.9 = f32[100,1,28,28]{3,2,1,0} bitcast(f32[100,28,28,1]{2,1,0,3} %multiply.0), metadata={op_type="Transpose" op_name="owlnode28-0-TransposeNHWCToNCHW-LayoutOptimizer"}
}

%fused_computation.6 (param_0.15: f32[1,10]) -> f32[100,10] {
  %param_0.15 = f32[1,10]{1,0} parameter(0)
  %bitcast.11 = f32[10]{0} bitcast(f32[1,10]{1,0} %param_0.15), metadata={op_type="Add" op_name="owlnode50"}
  ROOT %broadcast.10 = f32[100,10]{1,0} broadcast(f32[10]{0} %bitcast.11), dimensions={1}, metadata={op_type="Add" op_name="owlnode50"}
}

ENTRY %cluster_1__XlaCompiledKernel_true__XlaNumConstantArgs_5__XlaNumResourceArgs_0_.76 (arg0.1: f32[32], arg1.2: f32[5,5,1,32], arg2.3: f32[6272,1024], arg3.4: f32[1,1024], arg4.5: f32[1024,10], arg5.6: f32[1,10], arg6.7: f32[100,28,28,1]) -> f32[100,10] {
  %arg5.6 = f32[1,10]{1,0} parameter(5), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.6 = f32[100,10]{1,0} fusion(f32[1,10]{1,0} %arg5.6), kind=kLoop, calls=%fused_computation.6, metadata={op_type="Add" op_name="owlnode50"}
  %arg6.7 = f32[100,28,28,1]{3,2,1,0} parameter(6), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.5 = f32[100,1,28,28]{3,2,1,0} fusion(f32[100,28,28,1]{3,2,1,0} %arg6.7), kind=kLoop, calls=%fused_computation.5, metadata={op_type="Transpose" op_name="owlnode28-0-TransposeNHWCToNCHW-LayoutOptimizer"}
  %arg1.2 = f32[5,5,1,32]{3,2,1,0} parameter(1), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %copy.4 = f32[5,5,1,32]{1,0,2,3} copy(f32[5,5,1,32]{3,2,1,0} %arg1.2), metadata={op_name="XLA_Args"}
  %custom-call.1 = (f32[100,32,28,28]{3,2,1,0}, u8[4712]{0}) custom-call(f32[100,1,28,28]{3,2,1,0} %fusion.5, f32[5,5,1,32]{1,0,2,3} %copy.4), window={size=5x5 pad=2_2x2_2}, dim_labels=bf01_01io->bf01, custom_call_target="__cudnn$convForward", metadata={op_type="Conv2D" op_name="owlnode28"}, backend_config="{\"algorithm\":\"1\",\"convResultScale\":1}"
  %get-tuple-element.1 = f32[100,32,28,28]{3,2,1,0} get-tuple-element((f32[100,32,28,28]{3,2,1,0}, u8[4712]{0}) %custom-call.1), index=0
  %arg0.1 = f32[32]{0} parameter(0), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.4 = f32[100,6272]{1,0} fusion(f32[100,32,28,28]{3,2,1,0} %get-tuple-element.1, f32[32]{0} %arg0.1), kind=kLoop, calls=%fused_computation.4, metadata={op_type="Reshape" op_name="owlnode36"}
  %arg2.3 = f32[6272,1024]{1,0} parameter(2), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %dot.35 = f32[100,1024]{1,0} dot(f32[100,6272]{1,0} %fusion.4, f32[6272,1024]{1,0} %arg2.3), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_type="MatMul" op_name="owlnode40"}
  %arg3.4 = f32[1,1024]{1,0} parameter(3), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.3 = f32[100,1024]{1,0} fusion(f32[100,1024]{1,0} %dot.35, f32[1,1024]{1,0} %arg3.4), kind=kLoop, calls=%fused_computation.3, metadata={op_type="Relu" op_name="owlnode44"}
  %arg4.5 = f32[1024,10]{1,0} parameter(4), parameter_replication={false}, metadata={op_name="XLA_Args"}
  %fusion.2 = f32[100,10]{1,0} fusion(f32[100,10]{1,0} %fusion.6, f32[100,1024]{1,0} %fusion.3, f32[1024,10]{1,0} %arg4.5), kind=kOutput, calls=%fused_computation.2, metadata={op_type="Add" op_name="owlnode50"}
  %constant_24 = f32[] constant(-inf), metadata={op_type="MaxPool" op_name="owlnode34"}
  %reduce.53 = f32[100]{0} reduce(f32[100,10]{1,0} %fusion.2, f32[] %constant_24), dimensions={1}, to_apply=%owlnode52-reduction.49, metadata={op_type="Max" op_name="owlnode52"}
  %fusion.1 = f32[100,10]{1,0} fusion(f32[100,10]{1,0} %fusion.2, f32[100]{0} %reduce.53), kind=kLoop, calls=%fused_computation.1, metadata={op_type="Exp" op_name="owlnode55"}
  %constant_30 = f32[] constant(0), metadata={op_type="Relu" op_name="owlnode32"}
  %reduce.67 = f32[100]{0} reduce(f32[100,10]{1,0} %fusion.1, f32[] %constant_30), dimensions={1}, to_apply=%owlnode57-reduction.63, metadata={op_type="Sum" op_name="owlnode57"}
  ROOT %fusion = f32[100,10]{1,0} fusion(f32[100,10]{1,0} %fusion.1, f32[100]{0} %reduce.67), kind=kLoop, calls=%fused_computation, metadata={op_type="Div" op_name="owlnode59"}
}

